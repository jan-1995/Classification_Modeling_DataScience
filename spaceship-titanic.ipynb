{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" #  <center>SpaceShip-Titanic Tutorial </center>\n \n1. In this tutorial, we are going to be **making a classification model to make predictions on the spaceship titanic dataset**. Being a beginners tutorial, all you need to have some basic knowledge in statictics and python.\n \n2. In this tutorial, we will be making use of Pandas Profiling to make detailed statistical reports about the dataset, you'll see for yourself the ultimate power of **Pandas Profiling** and its use for **fast pacing the construction of Data reports**, something we (data scientists) used to do by importing a libraries and creating instances.\n\n3. Although we will be using off the shelf classification algorithms and trainging them, we will no tbe going into detail of how the algorithms work, although **you'll be learning how to make model pipelines and creating dictionaries in model construction**.\n \n3. In the end, if you like this tutorial, **it'll mean alot to me if you upvote the tutorial**, I also have some more awesome content dropping **SOON** with more indepth statistical analysis into complex datasets.\n \n # Table of Contents\n    \n<a id=\"toc\"></a>\n \n - [1. Taking a Look at the Data and make some notes](#1)\n - [2. Do fast EDA using Pandas Profiling](#2)\n - [4. Data Preprocessing and Feature Engineeing](#4)\n - [5. Model Construction](#5)\n - [6. Predictions](#6)\n - [7. Submission](#7)\n \n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Take a Look at our Data and Make some Notes","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.451775Z","iopub.execute_input":"2022-09-06T15:02:14.452542Z","iopub.status.idle":"2022-09-06T15:02:14.457386Z","shell.execute_reply.started":"2022-09-06T15:02:14.452503Z","shell.execute_reply":"2022-09-06T15:02:14.456165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Read in and Take a Look at our data\ndf=pd.read_csv('../input/spaceship-titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.458939Z","iopub.execute_input":"2022-09-06T15:02:14.459624Z","iopub.status.idle":"2022-09-06T15:02:14.498326Z","shell.execute_reply.started":"2022-09-06T15:02:14.459584Z","shell.execute_reply":"2022-09-06T15:02:14.496772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.499891Z","iopub.execute_input":"2022-09-06T15:02:14.500311Z","iopub.status.idle":"2022-09-06T15:02:14.522868Z","shell.execute_reply.started":"2022-09-06T15:02:14.500269Z","shell.execute_reply":"2022-09-06T15:02:14.521917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##See whether we have any null values in our Dataset\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.524364Z","iopub.execute_input":"2022-09-06T15:02:14.524941Z","iopub.status.idle":"2022-09-06T15:02:14.541542Z","shell.execute_reply.started":"2022-09-06T15:02:14.524904Z","shell.execute_reply":"2022-09-06T15:02:14.540305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Well well well...... we do have a lot of null values in our dataset, hmmm..... we'll handle this later, let us look at the statistics of our dataset.... this is something i do to get an idea of the distribution of the data.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.543025Z","iopub.execute_input":"2022-09-06T15:02:14.543948Z","iopub.status.idle":"2022-09-06T15:02:14.579649Z","shell.execute_reply.started":"2022-09-06T15:02:14.543911Z","shell.execute_reply":"2022-09-06T15:02:14.578549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **My Notes**\n\n1. People ina  group are often families bt not always, _02 in the family ID then its a group travelling\n2. cabin is split up into deck number and side so we may want to split thatup\n3. Room Service, FOOD coURT, VR DECK SPA AND SHOPPING MALL ARE MONETARY VALUES \n4. Machine Learning Task is to predict transported\nIts a binary Classification Problem.\n\n\n\nTo Do\n1. Fill in Missing Values in the columns\n2. Also Balance out the dataset if imbalanced.\n3.Drop high cardinality columns\n\n\nIf you are doing a real data sceince project, you go to the client and speak to them about the data, you ask them about the relationships present in the dayta and about the variable they are trying to predict.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# **Do Fast EDA using Pandas Profiling**\n\nPandas Profiling is an extremely useful tool for y'all data science enthusiasts.  The pandas df.describe() function is handy yet a little basic for exploratory data analysis. pandas-profiling extends pandas DataFrame with df.profile_report(), which automatically generates a standardized univariate and multivariate report for data understanding.","metadata":{}},{"cell_type":"code","source":"!pip install pandas-profiling","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:14.580933Z","iopub.execute_input":"2022-09-06T15:02:14.581336Z","iopub.status.idle":"2022-09-06T15:02:25.973411Z","shell.execute_reply.started":"2022-09-06T15:02:14.581300Z","shell.execute_reply":"2022-09-06T15:02:25.971891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the library which we just installed\nfrom pandas_profiling import ProfileReport","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:25.975762Z","iopub.execute_input":"2022-09-06T15:02:25.977033Z","iopub.status.idle":"2022-09-06T15:02:25.982984Z","shell.execute_reply.started":"2022-09-06T15:02:25.976969Z","shell.execute_reply":"2022-09-06T15:02:25.981731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Call the function from the library and enter the dataframe you want to do EDA on\nprofile =ProfileReport(df,'Spaceship-Titanic')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:25.984686Z","iopub.execute_input":"2022-09-06T15:02:25.985438Z","iopub.status.idle":"2022-09-06T15:02:26.032895Z","shell.execute_reply.started":"2022-09-06T15:02:25.985351Z","shell.execute_reply":"2022-09-06T15:02:26.031486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What the Profile Report Gives you\n\n0. Type inference: detect the types of columns in a DataFrame\n1. Essentials: type, unique values, indication of missing values\n2. Quantile statistics: minimum value, Q1, median, Q3, maximum, range, interquartile range\n3. Descriptive statistics: mean, mode, standard deviation, sum, median absolute deviation, 4. 4. coefficient of variation, kurtosis, skewness\n5. Most frequent and extreme values\n6. Histograms: categorical and numerical\n7. Correlations: high correlation warnings, based on different correlation metrics (Spearman, 8. Pearson, Kendall, Cramér’s V, Phik)\n9. Missing values: through counts, matrix, heatmap and dendrograms\n10. Duplicate rows: list of the most common duplicated rows","metadata":{}},{"cell_type":"code","source":"profile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:26.035069Z","iopub.execute_input":"2022-09-06T15:02:26.036040Z","iopub.status.idle":"2022-09-06T15:02:29.796323Z","shell.execute_reply.started":"2022-09-06T15:02:26.035985Z","shell.execute_reply":"2022-09-06T15:02:29.795104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets focus on the Cabin Column: According to the definitions of each column provided in the dataset, The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. Lets divide this into Deck and Side Feature, for now I'll treat the deck number as not that relevant","metadata":{}},{"cell_type":"code","source":"# This function will later be used in the preprocessing pipeline, right now, its just being used\n# to see what we can do with the data\n\ndef split_cabin(x):\n  if len(str(x).split('/'))<3:\n\n    return ['Missing','Missing','Missing']\n\n\n  else:\n    return str(x).split('/')\n\n\ndf['TempCabin']=df['Cabin'].apply(lambda x: split_cabin(x))\ndf['Deck']=df['TempCabin'].apply(lambda x: x[0])\ndf['Side']=df['TempCabin'].apply(lambda x: x[2])","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.798160Z","iopub.execute_input":"2022-09-06T15:02:29.798527Z","iopub.status.idle":"2022-09-06T15:02:29.825927Z","shell.execute_reply.started":"2022-09-06T15:02:29.798494Z","shell.execute_reply":"2022-09-06T15:02:29.824921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Lets take another look at our data now, augmented with 3 more columns\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.827575Z","iopub.execute_input":"2022-09-06T15:02:29.827914Z","iopub.status.idle":"2022-09-06T15:02:29.853768Z","shell.execute_reply.started":"2022-09-06T15:02:29.827883Z","shell.execute_reply":"2022-09-06T15:02:29.852561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Drop the Temp Cabin Feature, this is a part of the data cleaning proces.\ndf.drop(['TempCabin'],axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.854897Z","iopub.execute_input":"2022-09-06T15:02:29.855326Z","iopub.status.idle":"2022-09-06T15:02:29.870627Z","shell.execute_reply.started":"2022-09-06T15:02:29.855287Z","shell.execute_reply":"2022-09-06T15:02:29.869076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.872179Z","iopub.execute_input":"2022-09-06T15:02:29.873150Z","iopub.status.idle":"2022-09-06T15:02:29.898269Z","shell.execute_reply.started":"2022-09-06T15:02:29.873098Z","shell.execute_reply":"2022-09-06T15:02:29.897032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we have about 14 feature columns and one label column, further data cleaning will be done to \n# Cater to the categorical features and numerical features, operations such as dummy creation for categorical variables\n#and data imputation for numerical variables.\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.899773Z","iopub.execute_input":"2022-09-06T15:02:29.900169Z","iopub.status.idle":"2022-09-06T15:02:29.921764Z","shell.execute_reply.started":"2022-09-06T15:02:29.900132Z","shell.execute_reply":"2022-09-06T15:02:29.920647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Data Preprocessing and Feature Engineering","metadata":{}},{"cell_type":"code","source":"##Create a preprocessing function to transform our dataset,\n##this preprocessing pipeline does it all i.e., feature engineering and Data cleaning  \ndef preprocessing (df):\n\n  #Fill missing values with missing\n  df['HomePlanet'].fillna('Missing',inplace=True)\n  df['CryoSleep'].fillna('Missing', inplace=True)\n  \n  #Cabin Preprocesing, extract Deck and S ide\n  df['TempCabin']=df['Cabin'].apply(lambda x: split_cabin(x))\n  df['Deck']=df['TempCabin'].apply(lambda x: x[0])\n  df['Side']=df['TempCabin'].apply(lambda x: x[2])\n  df.drop(['TempCabin','Cabin'], axis=1, inplace=True)\n  \n  #destination\n  # Fill null values with 'missing'\n  df['Destination'].fillna('Missing', inplace=True)\n  \n  #imputing the mean for age\n\n  df['Age'].fillna(df['Age'].mean(), inplace=True)\n  #VIP - drop na rows\n  df['VIP'].fillna('Missing', inplace=True)\n  \n  #Monetary spending columns:RoomService, FoodCourt, ShoppingMall, Spa,\n  #VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's\n  #many luxury amenities.\n  df['RoomService'].fillna(0, inplace=True)\n  df['FoodCourt'].fillna(0, inplace=True)\n  df['ShoppingMall'].fillna(0, inplace=True)\n  df['Spa'].fillna(0, inplace=True)\n  df['VRDeck'].fillna(0, inplace=True)\n  #Drop name = high cardinality\n  df.drop('Name', axis=1, inplace=True)\n  #Drop remaining rows\n\n  return df","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.923455Z","iopub.execute_input":"2022-09-06T15:02:29.923809Z","iopub.status.idle":"2022-09-06T15:02:29.934970Z","shell.execute_reply.started":"2022-09-06T15:02:29.923777Z","shell.execute_reply":"2022-09-06T15:02:29.933632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset from Model","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/spaceship-titanic/train.csv')\nabt=df.copy()\nabt.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.936668Z","iopub.execute_input":"2022-09-06T15:02:29.937797Z","iopub.status.idle":"2022-09-06T15:02:29.990825Z","shell.execute_reply.started":"2022-09-06T15:02:29.937745Z","shell.execute_reply":"2022-09-06T15:02:29.989594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Call the Preprocessing function on your trainuing dataset.\npreprocessing(abt)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:29.992434Z","iopub.execute_input":"2022-09-06T15:02:29.992801Z","iopub.status.idle":"2022-09-06T15:02:30.055522Z","shell.execute_reply.started":"2022-09-06T15:02:29.992767Z","shell.execute_reply":"2022-09-06T15:02:30.054169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#8693 unique records\n##We see that we have reduced the entries and have also dropped some columns\nabt.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.057543Z","iopub.execute_input":"2022-09-06T15:02:30.058352Z","iopub.status.idle":"2022-09-06T15:02:30.077594Z","shell.execute_reply.started":"2022-09-06T15:02:30.058299Z","shell.execute_reply":"2022-09-06T15:02:30.076082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Model Construction\n\n- Feature and Target values - X, y\n- One hot encode any categorical features\n- Train, holdout split\n- Train on a bunch of algos","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n#To check whether our dataset is balanced or not \nfrom matplotlib import pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.079633Z","iopub.execute_input":"2022-09-06T15:02:30.080102Z","iopub.status.idle":"2022-09-06T15:02:30.086254Z","shell.execute_reply.started":"2022-09-06T15:02:30.080055Z","shell.execute_reply":"2022-09-06T15:02:30.084719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating labels and features columns\nX= abt.drop(['Transported','PassengerId'],axis=1)\n#One hot encode\nX=pd.get_dummies(X)\ny=abt['Transported']","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.087518Z","iopub.execute_input":"2022-09-06T15:02:30.088723Z","iopub.status.idle":"2022-09-06T15:02:30.116975Z","shell.execute_reply.started":"2022-09-06T15:02:30.088684Z","shell.execute_reply":"2022-09-06T15:02:30.115765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train-Test-Split Partitioning**","metadata":{}},{"cell_type":"code","source":"#Create training and testing partitions\nX_train, X_test, y_train, y_test=train_test_split(X, y , test_size=0.3, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.119006Z","iopub.execute_input":"2022-09-06T15:02:30.119426Z","iopub.status.idle":"2022-09-06T15:02:30.130429Z","shell.execute_reply.started":"2022-09-06T15:02:30.119388Z","shell.execute_reply":"2022-09-06T15:02:30.129389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Transported', data=df)\n##It looks like our dataset is pretty much balanced, so what happens is that when we are working on binary classification problems,\n##we would want our dataset to be balanced with equal numbers of booleans\n##For a reasobnably well performing model","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.132341Z","iopub.execute_input":"2022-09-06T15:02:30.132938Z","iopub.status.idle":"2022-09-06T15:02:30.179782Z","shell.execute_reply.started":"2022-09-06T15:02:30.132887Z","shell.execute_reply":"2022-09-06T15:02:30.178345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a look at our training data\nX_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.181324Z","iopub.execute_input":"2022-09-06T15:02:30.181727Z","iopub.status.idle":"2022-09-06T15:02:30.198871Z","shell.execute_reply.started":"2022-09-06T15:02:30.181689Z","shell.execute_reply":"2022-09-06T15:02:30.197593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.200505Z","iopub.execute_input":"2022-09-06T15:02:30.200990Z","iopub.status.idle":"2022-09-06T15:02:30.209069Z","shell.execute_reply.started":"2022-09-06T15:02:30.200942Z","shell.execute_reply":"2022-09-06T15:02:30.207772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.211110Z","iopub.execute_input":"2022-09-06T15:02:30.211510Z","iopub.status.idle":"2022-09-06T15:02:30.217523Z","shell.execute_reply.started":"2022-09-06T15:02:30.211474Z","shell.execute_reply":"2022-09-06T15:02:30.216291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning Algorithms to Use\n\n **1**. Grid Search CV: GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters. Learn out more in depth about Grid Search CV using the following link : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n\n**2**. Random Forest Classifier and Gradient Boosting Classifier which are both ensemble learning algorithms, find out more about them using this link : https://scikit-learn.org/stable/modules/ensemble.html#","metadata":{}},{"cell_type":"code","source":"##Setup our Machine Learnig Pipelines\npipelines={\n    'rf':make_pipeline(StandardScaler(),RandomForestClassifier(random_state=1234)),\n    'gb':make_pipeline(StandardScaler(),GradientBoostingClassifier(random_state=1234))\n\n}\n\n##The standard scaler essentially does x-mu divided by sigma, it scales the dataset to center around zero mean","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.219390Z","iopub.execute_input":"2022-09-06T15:02:30.220154Z","iopub.status.idle":"2022-09-06T15:02:30.229519Z","shell.execute_reply.started":"2022-09-06T15:02:30.220105Z","shell.execute_reply":"2022-09-06T15:02:30.228449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid={\n    'rf':{\n        'randomforestclassifier__n_estimators':[100,200,300]\n    },\n    'gb':{\n        'gradientboostingclassifier__n_estimators':[100,200,300]\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.231037Z","iopub.execute_input":"2022-09-06T15:02:30.231659Z","iopub.status.idle":"2022-09-06T15:02:30.239705Z","shell.execute_reply.started":"2022-09-06T15:02:30.231624Z","shell.execute_reply":"2022-09-06T15:02:30.238440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RandomForestClassifier().get_params()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.241786Z","iopub.execute_input":"2022-09-06T15:02:30.242594Z","iopub.status.idle":"2022-09-06T15:02:30.253122Z","shell.execute_reply.started":"2022-09-06T15:02:30.242543Z","shell.execute_reply":"2022-09-06T15:02:30.252279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning Loop\nWere tuning, we tune using a gridsearch cv its going to scan across our tuning grid like our hyperparameters and also perform crossvalidation","metadata":{}},{"cell_type":"code","source":"pipelines.items()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.254286Z","iopub.execute_input":"2022-09-06T15:02:30.255195Z","iopub.status.idle":"2022-09-06T15:02:30.269248Z","shell.execute_reply.started":"2022-09-06T15:02:30.255160Z","shell.execute_reply":"2022-09-06T15:02:30.268172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a blank dictionary to hold models\nfit_models={}\n#Loop through al the algos\nfor algo, pipeline in pipelines.items():\n  print(f'training the {algo} model.')\n  #Create new grid search cv class\n  model=GridSearchCV(pipeline, grid[algo], n_jobs=-1, cv=10)\n  # Train the model\n  model.fit(X_train, y_train)\n  #Store the results inside of our dictionary\n  fit_models[algo]=model","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:02:30.270869Z","iopub.execute_input":"2022-09-06T15:02:30.271431Z","iopub.status.idle":"2022-09-06T15:03:08.643921Z","shell.execute_reply.started":"2022-09-06T15:02:30.271396Z","shell.execute_reply":"2022-09-06T15:03:08.642635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model on the Test Partition\n- Grab the testing data","metadata":{}},{"cell_type":"code","source":"fit_models","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.645384Z","iopub.execute_input":"2022-09-06T15:03:08.645739Z","iopub.status.idle":"2022-09-06T15:03:08.669916Z","shell.execute_reply.started":"2022-09-06T15:03:08.645707Z","shell.execute_reply":"2022-09-06T15:03:08.668671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.672142Z","iopub.execute_input":"2022-09-06T15:03:08.673350Z","iopub.status.idle":"2022-09-06T15:03:08.678885Z","shell.execute_reply.started":"2022-09-06T15:03:08.673302Z","shell.execute_reply":"2022-09-06T15:03:08.677994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the performance of the models\n\nfor algo, model in fit_models.items():\n  y_hat=model.predict(X_test)\n  print(accuracy_score(y_test, y_hat))\n  accuracy=accuracy_score(y_test,y_hat)\n  recall=recall_score(y_test,y_hat)\n  precision=precision_score(y_test,y_hat)\n  print(f'Metrics for {algo}: accuracy -{accuracy}, recall-{recall}, precision-{precision}')\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.680697Z","iopub.execute_input":"2022-09-06T15:03:08.681523Z","iopub.status.idle":"2022-09-06T15:03:08.928030Z","shell.execute_reply.started":"2022-09-06T15:03:08.681478Z","shell.execute_reply":"2022-09-06T15:03:08.926907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.929573Z","iopub.execute_input":"2022-09-06T15:03:08.929965Z","iopub.status.idle":"2022-09-06T15:03:08.938409Z","shell.execute_reply.started":"2022-09-06T15:03:08.929930Z","shell.execute_reply":"2022-09-06T15:03:08.936958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Best Model","metadata":{}},{"cell_type":"code","source":"import pickle\nwith open('gradientboosted.pkl','wb') as f:\n  pickle.dump(fit_models['gb'],f)\nwith open('gradientboosted.pkl','rb') as f:\n  reloaded_model=pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.939988Z","iopub.execute_input":"2022-09-06T15:03:08.940426Z","iopub.status.idle":"2022-09-06T15:03:08.964771Z","shell.execute_reply.started":"2022-09-06T15:03:08.940380Z","shell.execute_reply":"2022-09-06T15:03:08.963486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n# Predict our Test Data","metadata":{}},{"cell_type":"code","source":"#Read in the test CSV dataset\ntest_df=pd.read_csv('../input/spaceship-titanic/test.csv')\n#Create a Deep copy of the test dataset\nabt_test=test_df.copy()\n#Run the deep copy through the preprocessing pipeline\npreprocessing(abt_test)\n#One hot encode categorical data\nabt_test=pd.get_dummies(abt_test.drop('PassengerId', axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:08.966576Z","iopub.execute_input":"2022-09-06T15:03:08.967612Z","iopub.status.idle":"2022-09-06T15:03:09.020637Z","shell.execute_reply.started":"2022-09-06T15:03:08.967566Z","shell.execute_reply":"2022-09-06T15:03:09.019235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abt_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:09.022444Z","iopub.execute_input":"2022-09-06T15:03:09.022810Z","iopub.status.idle":"2022-09-06T15:03:09.048324Z","shell.execute_reply.started":"2022-09-06T15:03:09.022777Z","shell.execute_reply":"2022-09-06T15:03:09.047352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat_test=fit_models['gb'].predict(abt_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:09.060386Z","iopub.execute_input":"2022-09-06T15:03:09.061218Z","iopub.status.idle":"2022-09-06T15:03:09.085045Z","shell.execute_reply.started":"2022-09-06T15:03:09.061172Z","shell.execute_reply":"2022-09-06T15:03:09.083889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.DataFrame([test_df['PassengerId'],yhat_test]).T\nsubmission.columns=['PassengerId','Transported']","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:09.086552Z","iopub.execute_input":"2022-09-06T15:03:09.086915Z","iopub.status.idle":"2022-09-06T15:03:09.244929Z","shell.execute_reply.started":"2022-09-06T15:03:09.086882Z","shell.execute_reply":"2022-09-06T15:03:09.243658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:09.246351Z","iopub.execute_input":"2022-09-06T15:03:09.247288Z","iopub.status.idle":"2022-09-06T15:03:09.259738Z","shell.execute_reply.started":"2022-09-06T15:03:09.247244Z","shell.execute_reply":"2022-09-06T15:03:09.258504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Submitting Our Work","metadata":{}},{"cell_type":"code","source":"# Convert submission dataframe to a csv file (A comma separated Value File)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:03:09.262283Z","iopub.execute_input":"2022-09-06T15:03:09.263312Z","iopub.status.idle":"2022-09-06T15:03:09.276981Z","shell.execute_reply.started":"2022-09-06T15:03:09.263253Z","shell.execute_reply":"2022-09-06T15:03:09.275894Z"},"trusted":true},"execution_count":null,"outputs":[]}]}